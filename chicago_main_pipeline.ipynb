{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iyQtljP-qPHY",
    "outputId": "8a87f212-7215-410d-dcd3-d12a2fb5eed3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Use the latest version of pip.\n",
    "# !pip install --upgrade pip\n",
    "# !pip install --upgrade \"tfx[kfp]\"\n",
    "# !pip install --upgrade tensorflow_transform\n",
    "\n",
    "import os\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tfx import v1 as tfx\n",
    "from tfx.types import Channel, standard_artifacts\n",
    "from tfx.dsl.components.common.resolver import Resolver\n",
    "from tfx.components import Evaluator\n",
    "from tfx import v1 as tfx\n",
    "from tfx_bsl.public import tfxio\n",
    "from tfx.orchestration.pipeline import Pipeline\n",
    "from tfx.proto import trainer_pb2, pusher_pb2\n",
    "from tfx.types import standard_artifacts\n",
    "import tensorflow_model_analysis as tfma\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "kZQA0KrfXCvU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_SveIKxaENu"
   },
   "source": [
    "### Check the package versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xd-iP9wEaENu",
    "outputId": "4582eaeb-e6a1-46a5-ca31-ada3a64359c7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.15.1\n",
      "TFX version: 1.15.1\n",
      "KFP version: 1.8.22\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "from tfx import v1 as tfx\n",
    "print('TFX version: {}'.format(tfx.__version__))\n",
    "import kfp\n",
    "print('KFP version: {}'.format(kfp.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "EcUseqJaE2XN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "GOOGLE_CLOUD_PROJECT = 'brldi-gcpcapabilities-ai-audit'\n",
    "GOOGLE_CLOUD_REGION = 'us-central1'\n",
    "GCS_BUCKET_NAME = 'finalrundemo1chicago'\n",
    "FILE_NAME ='Final_Chicago_Train.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAaCPLjgiJrO"
   },
   "source": [
    "#### Set `gcloud` to use your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VkWdxe4TXRHk",
    "outputId": "bd93c29f-9870-45da-eab2-7180b8b01eca",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brldi-gcpcapabilities-ai-audit'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " {GOOGLE_CLOUD_PROJECT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_x_8xoj6k0L"
   },
   "source": [
    "### Set up Global variables for model serving locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CPN6UL5CazNy",
    "outputId": "75956013-1a23-4e03-9162-99b0709e2dcf",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCS directories created:\n",
      "gs://finalrundemo1chicago/chicago-gcp-vertex-pipelines/pipeline_root/\n",
      "gs://finalrundemo1chicago/chicago-gcp-vertex-pipelines/pipeline_module/\n",
      "gs://finalrundemo1chicago/chicago-gcp-vertex-pipelines/data/\n",
      "gs://finalrundemo1chicago/chicago-gcp-vertex-pipelines/serving_model/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize the Google Cloud Storage client\n",
    "client = storage.Client()\n",
    "\n",
    "\n",
    "PIPELINE_NAME = 'chicago-gcp-vertex-pipelines'\n",
    "\n",
    "# Function to create directories in GCS\n",
    "def create_gcs_directory(bucket_name, path):\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(path)\n",
    "    blob.upload_from_string('', content_type='application/x-www-form-urlencoded;charset=UTF-8')\n",
    "\n",
    "# Paths for pipeline artifacts\n",
    "PIPELINE_ROOT = f'gs://{GCS_BUCKET_NAME}/{PIPELINE_NAME}/pipeline_root/'\n",
    "MODULE_ROOT = f'gs://{GCS_BUCKET_NAME}/{PIPELINE_NAME}/pipeline_module/'\n",
    "DATA_ROOT = f'gs://{GCS_BUCKET_NAME}/{PIPELINE_NAME}/data/'\n",
    "SERVING_MODEL_DIR = f'gs://{GCS_BUCKET_NAME}/{PIPELINE_NAME}/serving_model/'\n",
    "\n",
    "# Create directories\n",
    "create_gcs_directory(GCS_BUCKET_NAME, f'{PIPELINE_NAME}/pipeline_root/')\n",
    "create_gcs_directory(GCS_BUCKET_NAME, f'{PIPELINE_NAME}/pipeline_module/')\n",
    "create_gcs_directory(GCS_BUCKET_NAME, f'{PIPELINE_NAME}/data/')\n",
    "create_gcs_directory(GCS_BUCKET_NAME, f'{PIPELINE_NAME}/serving_model/')\n",
    "\n",
    "print('GCS directories created:')\n",
    "print(PIPELINE_ROOT)\n",
    "print(MODULE_ROOT)\n",
    "print(DATA_ROOT)\n",
    "print(SERVING_MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11J7XiCq6AFP"
   },
   "source": [
    "We need to make our own copy of the dataset. Because TFX ExampleGen reads\n",
    "inputs from a directory, we need to create a directory and copy dataset to it\n",
    "on GCS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASpoNmxKSQjI"
   },
   "source": [
    "Take a quick look at the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_key</th>\n",
       "      <th>taxi_id</th>\n",
       "      <th>trip_start_timestamp</th>\n",
       "      <th>trip_end_timestamp</th>\n",
       "      <th>trip_seconds</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>pickup_census_tract</th>\n",
       "      <th>dropoff_census_tract</th>\n",
       "      <th>pickup_community_area</th>\n",
       "      <th>dropoff_community_area</th>\n",
       "      <th>...</th>\n",
       "      <th>extras</th>\n",
       "      <th>trip_total</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>company</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_location</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f446a00d13583893f8dff96d5e7f5f1c344b3b0f</td>\n",
       "      <td>72efecb1648eb2bb2584e1add78527ffa37dfef82aba84...</td>\n",
       "      <td>2019-03-07 23:45:00+00:00</td>\n",
       "      <td>2019-03-08 00:00:00+00:00</td>\n",
       "      <td>600</td>\n",
       "      <td>3.20</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.60</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Star North Management LLC</td>\n",
       "      <td>41.874005</td>\n",
       "      <td>-87.663518</td>\n",
       "      <td>POINT (-87.6635175498 41.874005383)</td>\n",
       "      <td>41.922761</td>\n",
       "      <td>-87.699155</td>\n",
       "      <td>POINT (-87.69915534320002 41.9227606205)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b0e63c3f94bdcc5087075ce68fd327b3a077ae13</td>\n",
       "      <td>e585218a9f728ba533db40a74237a1caa23f01212f0fa8...</td>\n",
       "      <td>2019-03-10 04:00:00+00:00</td>\n",
       "      <td>2019-03-10 04:00:00+00:00</td>\n",
       "      <td>420</td>\n",
       "      <td>1.60</td>\n",
       "      <td>17031081700</td>\n",
       "      <td>17031833000</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.50</td>\n",
       "      <td>Cash</td>\n",
       "      <td>KOAM Taxi Association</td>\n",
       "      <td>41.892042</td>\n",
       "      <td>-87.631864</td>\n",
       "      <td>POINT (-87.6318639497 41.8920421365)</td>\n",
       "      <td>41.885281</td>\n",
       "      <td>-87.657233</td>\n",
       "      <td>POINT (-87.6572331997 41.8852813201)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fa7664fb4a363a891374554e01528e5da219f7c4</td>\n",
       "      <td>249ef6f75a49feebb50f4bc68cf7ba703c4006498c63b6...</td>\n",
       "      <td>2019-01-08 11:30:00+00:00</td>\n",
       "      <td>2019-01-08 11:30:00+00:00</td>\n",
       "      <td>451</td>\n",
       "      <td>1.56</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>33</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.75</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Chicago Carriage Cab Corp</td>\n",
       "      <td>41.857184</td>\n",
       "      <td>-87.620335</td>\n",
       "      <td>POINT (-87.6203346241 41.8571838585)</td>\n",
       "      <td>41.878866</td>\n",
       "      <td>-87.625192</td>\n",
       "      <td>POINT (-87.6251921424 41.8788655841)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e648cfac8d404da2749fdcdd9e566a1b9f026bed</td>\n",
       "      <td>7ec17a8416dca01eef674860959ae50b91aeecb1b417b0...</td>\n",
       "      <td>2019-04-05 17:00:00+00:00</td>\n",
       "      <td>2019-04-05 17:15:00+00:00</td>\n",
       "      <td>540</td>\n",
       "      <td>0.60</td>\n",
       "      <td>17031839100</td>\n",
       "      <td>17031081700</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.75</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Star North Management LLC</td>\n",
       "      <td>41.880994</td>\n",
       "      <td>-87.632746</td>\n",
       "      <td>POINT (-87.6327464887 41.8809944707)</td>\n",
       "      <td>41.892042</td>\n",
       "      <td>-87.631864</td>\n",
       "      <td>POINT (-87.6318639497 41.8920421365)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25abfc3959ff900d456804165c03351c1aff647b</td>\n",
       "      <td>b2917f6d0a6e1407152c6fa9c392d03ca840fd719c8062...</td>\n",
       "      <td>2019-02-13 15:15:00+00:00</td>\n",
       "      <td>2019-02-13 16:15:00+00:00</td>\n",
       "      <td>3540</td>\n",
       "      <td>19.80</td>\n",
       "      <td>17031980000</td>\n",
       "      <td>17031330100</td>\n",
       "      <td>76</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>62.20</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Medallion Leasin</td>\n",
       "      <td>41.979071</td>\n",
       "      <td>-87.903040</td>\n",
       "      <td>POINT (-87.9030396611 41.9790708201)</td>\n",
       "      <td>41.859350</td>\n",
       "      <td>-87.617358</td>\n",
       "      <td>POINT (-87.6173580061 41.859349715)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>34f2e523be8c72cc011aa4a9dd50df3063ddb4b8</td>\n",
       "      <td>9472ac2ed88284fd6d87425e35fbc13bd462a38f594b1b...</td>\n",
       "      <td>2019-04-12 03:45:00+00:00</td>\n",
       "      <td>2019-04-12 04:00:00+00:00</td>\n",
       "      <td>1260</td>\n",
       "      <td>1.10</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>8</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>57.60</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Taxi Affiliation Services</td>\n",
       "      <td>41.899602</td>\n",
       "      <td>-87.633308</td>\n",
       "      <td>POINT (-87.6333080367 41.899602111)</td>\n",
       "      <td>41.980264</td>\n",
       "      <td>-87.913625</td>\n",
       "      <td>POINT (-87.913624596 41.9802643146)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>2d21ee9e7a3b7f0e2b08592d7a39788d49865477</td>\n",
       "      <td>d092ee0cb44dc6fa5f6757a7ef07181c4ee3d25eed0bc4...</td>\n",
       "      <td>2019-04-26 13:30:00+00:00</td>\n",
       "      <td>2019-04-26 13:45:00+00:00</td>\n",
       "      <td>240</td>\n",
       "      <td>0.60</td>\n",
       "      <td>17031839100</td>\n",
       "      <td>17031320400</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.75</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Top Cab Affiliation</td>\n",
       "      <td>41.880994</td>\n",
       "      <td>-87.632746</td>\n",
       "      <td>POINT (-87.6327464887 41.8809944707)</td>\n",
       "      <td>41.877406</td>\n",
       "      <td>-87.621972</td>\n",
       "      <td>POINT (-87.6219716519 41.8774061234)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>8de0c36894b3353ff165f7e455399745bf08456f</td>\n",
       "      <td>0c0b18d1759f9e7dc42dcec3eaccc1531e18dce30b5cd3...</td>\n",
       "      <td>2019-04-03 19:30:00+00:00</td>\n",
       "      <td>2019-04-03 19:45:00+00:00</td>\n",
       "      <td>1140</td>\n",
       "      <td>5.20</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.75</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Taxi Affiliation Services</td>\n",
       "      <td>41.899602</td>\n",
       "      <td>-87.633308</td>\n",
       "      <td>POINT (-87.6333080367 41.899602111)</td>\n",
       "      <td>41.965812</td>\n",
       "      <td>-87.655879</td>\n",
       "      <td>POINT (-87.6558787862 41.96581197)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>9a81669dd209249baea02fc8613abf1c6ccbccfc</td>\n",
       "      <td>7e25041185a6cc504dd6e2f71691c595aeae45c4b86247...</td>\n",
       "      <td>2019-03-06 21:30:00+00:00</td>\n",
       "      <td>2019-03-06 21:45:00+00:00</td>\n",
       "      <td>1140</td>\n",
       "      <td>13.50</td>\n",
       "      <td>17031980000</td>\n",
       "      <td>17031831000</td>\n",
       "      <td>76</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>44.25</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Top Cab Affiliation</td>\n",
       "      <td>41.979071</td>\n",
       "      <td>-87.903040</td>\n",
       "      <td>POINT (-87.9030396611 41.9790708201)</td>\n",
       "      <td>41.916005</td>\n",
       "      <td>-87.675095</td>\n",
       "      <td>POINT (-87.6750951155 41.9160052737)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>e3cff99b3c3fa92a233c94902b95e0baa75085ab</td>\n",
       "      <td>c2f602a679cd26fd3fd117bc3964bd3501a66f67d3da6e...</td>\n",
       "      <td>2019-03-08 03:15:00+00:00</td>\n",
       "      <td>2019-03-08 03:15:00+00:00</td>\n",
       "      <td>181</td>\n",
       "      <td>0.60</td>\n",
       "      <td>17031081100</td>\n",
       "      <td>17031081700</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.75</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Chicago Carriage Cab Corp</td>\n",
       "      <td>41.900221</td>\n",
       "      <td>-87.629105</td>\n",
       "      <td>POINT (-87.6291051864 41.9002212967)</td>\n",
       "      <td>41.892042</td>\n",
       "      <td>-87.631864</td>\n",
       "      <td>POINT (-87.6318639497 41.8920421365)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     unique_key  \\\n",
       "0      f446a00d13583893f8dff96d5e7f5f1c344b3b0f   \n",
       "1      b0e63c3f94bdcc5087075ce68fd327b3a077ae13   \n",
       "2      fa7664fb4a363a891374554e01528e5da219f7c4   \n",
       "3      e648cfac8d404da2749fdcdd9e566a1b9f026bed   \n",
       "4      25abfc3959ff900d456804165c03351c1aff647b   \n",
       "...                                         ...   \n",
       "39995  34f2e523be8c72cc011aa4a9dd50df3063ddb4b8   \n",
       "39996  2d21ee9e7a3b7f0e2b08592d7a39788d49865477   \n",
       "39997  8de0c36894b3353ff165f7e455399745bf08456f   \n",
       "39998  9a81669dd209249baea02fc8613abf1c6ccbccfc   \n",
       "39999  e3cff99b3c3fa92a233c94902b95e0baa75085ab   \n",
       "\n",
       "                                                 taxi_id  \\\n",
       "0      72efecb1648eb2bb2584e1add78527ffa37dfef82aba84...   \n",
       "1      e585218a9f728ba533db40a74237a1caa23f01212f0fa8...   \n",
       "2      249ef6f75a49feebb50f4bc68cf7ba703c4006498c63b6...   \n",
       "3      7ec17a8416dca01eef674860959ae50b91aeecb1b417b0...   \n",
       "4      b2917f6d0a6e1407152c6fa9c392d03ca840fd719c8062...   \n",
       "...                                                  ...   \n",
       "39995  9472ac2ed88284fd6d87425e35fbc13bd462a38f594b1b...   \n",
       "39996  d092ee0cb44dc6fa5f6757a7ef07181c4ee3d25eed0bc4...   \n",
       "39997  0c0b18d1759f9e7dc42dcec3eaccc1531e18dce30b5cd3...   \n",
       "39998  7e25041185a6cc504dd6e2f71691c595aeae45c4b86247...   \n",
       "39999  c2f602a679cd26fd3fd117bc3964bd3501a66f67d3da6e...   \n",
       "\n",
       "           trip_start_timestamp        trip_end_timestamp  trip_seconds  \\\n",
       "0     2019-03-07 23:45:00+00:00 2019-03-08 00:00:00+00:00           600   \n",
       "1     2019-03-10 04:00:00+00:00 2019-03-10 04:00:00+00:00           420   \n",
       "2     2019-01-08 11:30:00+00:00 2019-01-08 11:30:00+00:00           451   \n",
       "3     2019-04-05 17:00:00+00:00 2019-04-05 17:15:00+00:00           540   \n",
       "4     2019-02-13 15:15:00+00:00 2019-02-13 16:15:00+00:00          3540   \n",
       "...                         ...                       ...           ...   \n",
       "39995 2019-04-12 03:45:00+00:00 2019-04-12 04:00:00+00:00          1260   \n",
       "39996 2019-04-26 13:30:00+00:00 2019-04-26 13:45:00+00:00           240   \n",
       "39997 2019-04-03 19:30:00+00:00 2019-04-03 19:45:00+00:00          1140   \n",
       "39998 2019-03-06 21:30:00+00:00 2019-03-06 21:45:00+00:00          1140   \n",
       "39999 2019-03-08 03:15:00+00:00 2019-03-08 03:15:00+00:00           181   \n",
       "\n",
       "       trip_miles  pickup_census_tract  dropoff_census_tract  \\\n",
       "0            3.20                 <NA>                  <NA>   \n",
       "1            1.60          17031081700           17031833000   \n",
       "2            1.56                 <NA>                  <NA>   \n",
       "3            0.60          17031839100           17031081700   \n",
       "4           19.80          17031980000           17031330100   \n",
       "...           ...                  ...                   ...   \n",
       "39995        1.10                 <NA>                  <NA>   \n",
       "39996        0.60          17031839100           17031320400   \n",
       "39997        5.20                 <NA>                  <NA>   \n",
       "39998       13.50          17031980000           17031831000   \n",
       "39999        0.60          17031081100           17031081700   \n",
       "\n",
       "       pickup_community_area  dropoff_community_area  ...  extras  trip_total  \\\n",
       "0                         28                      22  ...     2.0       15.60   \n",
       "1                          8                      28  ...     1.0        8.50   \n",
       "2                         33                      32  ...     0.0        7.75   \n",
       "3                         32                       8  ...     1.0        9.75   \n",
       "4                         76                      33  ...     4.0       62.20   \n",
       "...                      ...                     ...  ...     ...         ...   \n",
       "39995                      8                      76  ...     4.0       57.60   \n",
       "39996                     32                      32  ...     0.0        6.75   \n",
       "39997                      8                       3  ...     0.0       16.75   \n",
       "39998                     76                      22  ...     5.0       44.25   \n",
       "39999                      8                       8  ...     1.0        5.75   \n",
       "\n",
       "       payment_type                    company  pickup_latitude  \\\n",
       "0       Credit Card  Star North Management LLC        41.874005   \n",
       "1              Cash      KOAM Taxi Association        41.892042   \n",
       "2              Cash  Chicago Carriage Cab Corp        41.857184   \n",
       "3       Credit Card  Star North Management LLC        41.880994   \n",
       "4       Credit Card           Medallion Leasin        41.979071   \n",
       "...             ...                        ...              ...   \n",
       "39995   Credit Card  Taxi Affiliation Services        41.899602   \n",
       "39996   Credit Card        Top Cab Affiliation        41.880994   \n",
       "39997          Cash  Taxi Affiliation Services        41.899602   \n",
       "39998   Credit Card        Top Cab Affiliation        41.979071   \n",
       "39999          Cash  Chicago Carriage Cab Corp        41.900221   \n",
       "\n",
       "      pickup_longitude                       pickup_location  \\\n",
       "0           -87.663518   POINT (-87.6635175498 41.874005383)   \n",
       "1           -87.631864  POINT (-87.6318639497 41.8920421365)   \n",
       "2           -87.620335  POINT (-87.6203346241 41.8571838585)   \n",
       "3           -87.632746  POINT (-87.6327464887 41.8809944707)   \n",
       "4           -87.903040  POINT (-87.9030396611 41.9790708201)   \n",
       "...                ...                                   ...   \n",
       "39995       -87.633308   POINT (-87.6333080367 41.899602111)   \n",
       "39996       -87.632746  POINT (-87.6327464887 41.8809944707)   \n",
       "39997       -87.633308   POINT (-87.6333080367 41.899602111)   \n",
       "39998       -87.903040  POINT (-87.9030396611 41.9790708201)   \n",
       "39999       -87.629105  POINT (-87.6291051864 41.9002212967)   \n",
       "\n",
       "       dropoff_latitude  dropoff_longitude  \\\n",
       "0             41.922761         -87.699155   \n",
       "1             41.885281         -87.657233   \n",
       "2             41.878866         -87.625192   \n",
       "3             41.892042         -87.631864   \n",
       "4             41.859350         -87.617358   \n",
       "...                 ...                ...   \n",
       "39995         41.980264         -87.913625   \n",
       "39996         41.877406         -87.621972   \n",
       "39997         41.965812         -87.655879   \n",
       "39998         41.916005         -87.675095   \n",
       "39999         41.892042         -87.631864   \n",
       "\n",
       "                               dropoff_location  \n",
       "0      POINT (-87.69915534320002 41.9227606205)  \n",
       "1          POINT (-87.6572331997 41.8852813201)  \n",
       "2          POINT (-87.6251921424 41.8788655841)  \n",
       "3          POINT (-87.6318639497 41.8920421365)  \n",
       "4           POINT (-87.6173580061 41.859349715)  \n",
       "...                                         ...  \n",
       "39995       POINT (-87.913624596 41.9802643146)  \n",
       "39996      POINT (-87.6219716519 41.8774061234)  \n",
       "39997        POINT (-87.6558787862 41.96581197)  \n",
       "39998      POINT (-87.6750951155 41.9160052737)  \n",
       "39999      POINT (-87.6318639497 41.8920421365)  \n",
       "\n",
       "[40000 rows x 23 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "client = bigquery.Client()\n",
    "# Define the SQL query\n",
    "query = \"\"\"\n",
    "SELECT * FROM bigquery-public-data.chicago_taxi_trips.taxi_trips\n",
    "WHERE EXTRACT(year FROM trip_start_timestamp) > 2018 AND\n",
    "      trip_miles IS NOT NULL AND\n",
    "      trip_seconds IS NOT NULL\n",
    "ORDER BY RAND()\n",
    "LIMIT 40000\n",
    "\"\"\"\n",
    "\n",
    "# Run the query\n",
    "query_job = client.query(query)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = query_job.to_dataframe()\n",
    "\n",
    "# Display the DataFrame\n",
    "df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unique_key                    0\n",
       "taxi_id                       0\n",
       "trip_start_timestamp          0\n",
       "trip_end_timestamp            0\n",
       "trip_seconds                  0\n",
       "trip_miles                    0\n",
       "pickup_census_tract       13198\n",
       "dropoff_census_tract      13587\n",
       "pickup_community_area      2376\n",
       "dropoff_community_area     3535\n",
       "fare                          5\n",
       "tips                          5\n",
       "tolls                       149\n",
       "extras                        5\n",
       "trip_total                    5\n",
       "payment_type                  0\n",
       "company                       0\n",
       "pickup_latitude            2375\n",
       "pickup_longitude           2375\n",
       "pickup_location            2375\n",
       "dropoff_latitude           3422\n",
       "dropoff_longitude          3422\n",
       "dropoff_location           3422\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"trip_start_day\"] = df.trip_start_timestamp.apply(lambda x: x.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"trip_start_month\"] = df.trip_start_timestamp.apply(lambda x: x.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"trip_start_hour\"] = df.trip_start_timestamp.apply(lambda x: x.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_key</th>\n",
       "      <th>taxi_id</th>\n",
       "      <th>trip_start_timestamp</th>\n",
       "      <th>trip_end_timestamp</th>\n",
       "      <th>trip_seconds</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>pickup_census_tract</th>\n",
       "      <th>dropoff_census_tract</th>\n",
       "      <th>pickup_community_area</th>\n",
       "      <th>dropoff_community_area</th>\n",
       "      <th>...</th>\n",
       "      <th>company</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_location</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_location</th>\n",
       "      <th>trip_start_day</th>\n",
       "      <th>trip_start_month</th>\n",
       "      <th>trip_start_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f446a00d13583893f8dff96d5e7f5f1c344b3b0f</td>\n",
       "      <td>72efecb1648eb2bb2584e1add78527ffa37dfef82aba84...</td>\n",
       "      <td>2019-03-07 23:45:00+00:00</td>\n",
       "      <td>2019-03-08 00:00:00+00:00</td>\n",
       "      <td>600</td>\n",
       "      <td>3.20</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>Star North Management LLC</td>\n",
       "      <td>41.874005</td>\n",
       "      <td>-87.663518</td>\n",
       "      <td>POINT (-87.6635175498 41.874005383)</td>\n",
       "      <td>41.922761</td>\n",
       "      <td>-87.699155</td>\n",
       "      <td>POINT (-87.69915534320002 41.9227606205)</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b0e63c3f94bdcc5087075ce68fd327b3a077ae13</td>\n",
       "      <td>e585218a9f728ba533db40a74237a1caa23f01212f0fa8...</td>\n",
       "      <td>2019-03-10 04:00:00+00:00</td>\n",
       "      <td>2019-03-10 04:00:00+00:00</td>\n",
       "      <td>420</td>\n",
       "      <td>1.60</td>\n",
       "      <td>17031081700</td>\n",
       "      <td>17031833000</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>KOAM Taxi Association</td>\n",
       "      <td>41.892042</td>\n",
       "      <td>-87.631864</td>\n",
       "      <td>POINT (-87.6318639497 41.8920421365)</td>\n",
       "      <td>41.885281</td>\n",
       "      <td>-87.657233</td>\n",
       "      <td>POINT (-87.6572331997 41.8852813201)</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fa7664fb4a363a891374554e01528e5da219f7c4</td>\n",
       "      <td>249ef6f75a49feebb50f4bc68cf7ba703c4006498c63b6...</td>\n",
       "      <td>2019-01-08 11:30:00+00:00</td>\n",
       "      <td>2019-01-08 11:30:00+00:00</td>\n",
       "      <td>451</td>\n",
       "      <td>1.56</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>33</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>Chicago Carriage Cab Corp</td>\n",
       "      <td>41.857184</td>\n",
       "      <td>-87.620335</td>\n",
       "      <td>POINT (-87.6203346241 41.8571838585)</td>\n",
       "      <td>41.878866</td>\n",
       "      <td>-87.625192</td>\n",
       "      <td>POINT (-87.6251921424 41.8788655841)</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e648cfac8d404da2749fdcdd9e566a1b9f026bed</td>\n",
       "      <td>7ec17a8416dca01eef674860959ae50b91aeecb1b417b0...</td>\n",
       "      <td>2019-04-05 17:00:00+00:00</td>\n",
       "      <td>2019-04-05 17:15:00+00:00</td>\n",
       "      <td>540</td>\n",
       "      <td>0.60</td>\n",
       "      <td>17031839100</td>\n",
       "      <td>17031081700</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>Star North Management LLC</td>\n",
       "      <td>41.880994</td>\n",
       "      <td>-87.632746</td>\n",
       "      <td>POINT (-87.6327464887 41.8809944707)</td>\n",
       "      <td>41.892042</td>\n",
       "      <td>-87.631864</td>\n",
       "      <td>POINT (-87.6318639497 41.8920421365)</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25abfc3959ff900d456804165c03351c1aff647b</td>\n",
       "      <td>b2917f6d0a6e1407152c6fa9c392d03ca840fd719c8062...</td>\n",
       "      <td>2019-02-13 15:15:00+00:00</td>\n",
       "      <td>2019-02-13 16:15:00+00:00</td>\n",
       "      <td>3540</td>\n",
       "      <td>19.80</td>\n",
       "      <td>17031980000</td>\n",
       "      <td>17031330100</td>\n",
       "      <td>76</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>Medallion Leasin</td>\n",
       "      <td>41.979071</td>\n",
       "      <td>-87.903040</td>\n",
       "      <td>POINT (-87.9030396611 41.9790708201)</td>\n",
       "      <td>41.859350</td>\n",
       "      <td>-87.617358</td>\n",
       "      <td>POINT (-87.6173580061 41.859349715)</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>34f2e523be8c72cc011aa4a9dd50df3063ddb4b8</td>\n",
       "      <td>9472ac2ed88284fd6d87425e35fbc13bd462a38f594b1b...</td>\n",
       "      <td>2019-04-12 03:45:00+00:00</td>\n",
       "      <td>2019-04-12 04:00:00+00:00</td>\n",
       "      <td>1260</td>\n",
       "      <td>1.10</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>8</td>\n",
       "      <td>76</td>\n",
       "      <td>...</td>\n",
       "      <td>Taxi Affiliation Services</td>\n",
       "      <td>41.899602</td>\n",
       "      <td>-87.633308</td>\n",
       "      <td>POINT (-87.6333080367 41.899602111)</td>\n",
       "      <td>41.980264</td>\n",
       "      <td>-87.913625</td>\n",
       "      <td>POINT (-87.913624596 41.9802643146)</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>2d21ee9e7a3b7f0e2b08592d7a39788d49865477</td>\n",
       "      <td>d092ee0cb44dc6fa5f6757a7ef07181c4ee3d25eed0bc4...</td>\n",
       "      <td>2019-04-26 13:30:00+00:00</td>\n",
       "      <td>2019-04-26 13:45:00+00:00</td>\n",
       "      <td>240</td>\n",
       "      <td>0.60</td>\n",
       "      <td>17031839100</td>\n",
       "      <td>17031320400</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>Top Cab Affiliation</td>\n",
       "      <td>41.880994</td>\n",
       "      <td>-87.632746</td>\n",
       "      <td>POINT (-87.6327464887 41.8809944707)</td>\n",
       "      <td>41.877406</td>\n",
       "      <td>-87.621972</td>\n",
       "      <td>POINT (-87.6219716519 41.8774061234)</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>8de0c36894b3353ff165f7e455399745bf08456f</td>\n",
       "      <td>0c0b18d1759f9e7dc42dcec3eaccc1531e18dce30b5cd3...</td>\n",
       "      <td>2019-04-03 19:30:00+00:00</td>\n",
       "      <td>2019-04-03 19:45:00+00:00</td>\n",
       "      <td>1140</td>\n",
       "      <td>5.20</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>Taxi Affiliation Services</td>\n",
       "      <td>41.899602</td>\n",
       "      <td>-87.633308</td>\n",
       "      <td>POINT (-87.6333080367 41.899602111)</td>\n",
       "      <td>41.965812</td>\n",
       "      <td>-87.655879</td>\n",
       "      <td>POINT (-87.6558787862 41.96581197)</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>9a81669dd209249baea02fc8613abf1c6ccbccfc</td>\n",
       "      <td>7e25041185a6cc504dd6e2f71691c595aeae45c4b86247...</td>\n",
       "      <td>2019-03-06 21:30:00+00:00</td>\n",
       "      <td>2019-03-06 21:45:00+00:00</td>\n",
       "      <td>1140</td>\n",
       "      <td>13.50</td>\n",
       "      <td>17031980000</td>\n",
       "      <td>17031831000</td>\n",
       "      <td>76</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>Top Cab Affiliation</td>\n",
       "      <td>41.979071</td>\n",
       "      <td>-87.903040</td>\n",
       "      <td>POINT (-87.9030396611 41.9790708201)</td>\n",
       "      <td>41.916005</td>\n",
       "      <td>-87.675095</td>\n",
       "      <td>POINT (-87.6750951155 41.9160052737)</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>e3cff99b3c3fa92a233c94902b95e0baa75085ab</td>\n",
       "      <td>c2f602a679cd26fd3fd117bc3964bd3501a66f67d3da6e...</td>\n",
       "      <td>2019-03-08 03:15:00+00:00</td>\n",
       "      <td>2019-03-08 03:15:00+00:00</td>\n",
       "      <td>181</td>\n",
       "      <td>0.60</td>\n",
       "      <td>17031081100</td>\n",
       "      <td>17031081700</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>Chicago Carriage Cab Corp</td>\n",
       "      <td>41.900221</td>\n",
       "      <td>-87.629105</td>\n",
       "      <td>POINT (-87.6291051864 41.9002212967)</td>\n",
       "      <td>41.892042</td>\n",
       "      <td>-87.631864</td>\n",
       "      <td>POINT (-87.6318639497 41.8920421365)</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     unique_key  \\\n",
       "0      f446a00d13583893f8dff96d5e7f5f1c344b3b0f   \n",
       "1      b0e63c3f94bdcc5087075ce68fd327b3a077ae13   \n",
       "2      fa7664fb4a363a891374554e01528e5da219f7c4   \n",
       "3      e648cfac8d404da2749fdcdd9e566a1b9f026bed   \n",
       "4      25abfc3959ff900d456804165c03351c1aff647b   \n",
       "...                                         ...   \n",
       "39995  34f2e523be8c72cc011aa4a9dd50df3063ddb4b8   \n",
       "39996  2d21ee9e7a3b7f0e2b08592d7a39788d49865477   \n",
       "39997  8de0c36894b3353ff165f7e455399745bf08456f   \n",
       "39998  9a81669dd209249baea02fc8613abf1c6ccbccfc   \n",
       "39999  e3cff99b3c3fa92a233c94902b95e0baa75085ab   \n",
       "\n",
       "                                                 taxi_id  \\\n",
       "0      72efecb1648eb2bb2584e1add78527ffa37dfef82aba84...   \n",
       "1      e585218a9f728ba533db40a74237a1caa23f01212f0fa8...   \n",
       "2      249ef6f75a49feebb50f4bc68cf7ba703c4006498c63b6...   \n",
       "3      7ec17a8416dca01eef674860959ae50b91aeecb1b417b0...   \n",
       "4      b2917f6d0a6e1407152c6fa9c392d03ca840fd719c8062...   \n",
       "...                                                  ...   \n",
       "39995  9472ac2ed88284fd6d87425e35fbc13bd462a38f594b1b...   \n",
       "39996  d092ee0cb44dc6fa5f6757a7ef07181c4ee3d25eed0bc4...   \n",
       "39997  0c0b18d1759f9e7dc42dcec3eaccc1531e18dce30b5cd3...   \n",
       "39998  7e25041185a6cc504dd6e2f71691c595aeae45c4b86247...   \n",
       "39999  c2f602a679cd26fd3fd117bc3964bd3501a66f67d3da6e...   \n",
       "\n",
       "           trip_start_timestamp        trip_end_timestamp  trip_seconds  \\\n",
       "0     2019-03-07 23:45:00+00:00 2019-03-08 00:00:00+00:00           600   \n",
       "1     2019-03-10 04:00:00+00:00 2019-03-10 04:00:00+00:00           420   \n",
       "2     2019-01-08 11:30:00+00:00 2019-01-08 11:30:00+00:00           451   \n",
       "3     2019-04-05 17:00:00+00:00 2019-04-05 17:15:00+00:00           540   \n",
       "4     2019-02-13 15:15:00+00:00 2019-02-13 16:15:00+00:00          3540   \n",
       "...                         ...                       ...           ...   \n",
       "39995 2019-04-12 03:45:00+00:00 2019-04-12 04:00:00+00:00          1260   \n",
       "39996 2019-04-26 13:30:00+00:00 2019-04-26 13:45:00+00:00           240   \n",
       "39997 2019-04-03 19:30:00+00:00 2019-04-03 19:45:00+00:00          1140   \n",
       "39998 2019-03-06 21:30:00+00:00 2019-03-06 21:45:00+00:00          1140   \n",
       "39999 2019-03-08 03:15:00+00:00 2019-03-08 03:15:00+00:00           181   \n",
       "\n",
       "       trip_miles  pickup_census_tract  dropoff_census_tract  \\\n",
       "0            3.20                 <NA>                  <NA>   \n",
       "1            1.60          17031081700           17031833000   \n",
       "2            1.56                 <NA>                  <NA>   \n",
       "3            0.60          17031839100           17031081700   \n",
       "4           19.80          17031980000           17031330100   \n",
       "...           ...                  ...                   ...   \n",
       "39995        1.10                 <NA>                  <NA>   \n",
       "39996        0.60          17031839100           17031320400   \n",
       "39997        5.20                 <NA>                  <NA>   \n",
       "39998       13.50          17031980000           17031831000   \n",
       "39999        0.60          17031081100           17031081700   \n",
       "\n",
       "       pickup_community_area  dropoff_community_area  ...  \\\n",
       "0                         28                      22  ...   \n",
       "1                          8                      28  ...   \n",
       "2                         33                      32  ...   \n",
       "3                         32                       8  ...   \n",
       "4                         76                      33  ...   \n",
       "...                      ...                     ...  ...   \n",
       "39995                      8                      76  ...   \n",
       "39996                     32                      32  ...   \n",
       "39997                      8                       3  ...   \n",
       "39998                     76                      22  ...   \n",
       "39999                      8                       8  ...   \n",
       "\n",
       "                         company  pickup_latitude  pickup_longitude  \\\n",
       "0      Star North Management LLC        41.874005        -87.663518   \n",
       "1          KOAM Taxi Association        41.892042        -87.631864   \n",
       "2      Chicago Carriage Cab Corp        41.857184        -87.620335   \n",
       "3      Star North Management LLC        41.880994        -87.632746   \n",
       "4               Medallion Leasin        41.979071        -87.903040   \n",
       "...                          ...              ...               ...   \n",
       "39995  Taxi Affiliation Services        41.899602        -87.633308   \n",
       "39996        Top Cab Affiliation        41.880994        -87.632746   \n",
       "39997  Taxi Affiliation Services        41.899602        -87.633308   \n",
       "39998        Top Cab Affiliation        41.979071        -87.903040   \n",
       "39999  Chicago Carriage Cab Corp        41.900221        -87.629105   \n",
       "\n",
       "                            pickup_location  dropoff_latitude  \\\n",
       "0       POINT (-87.6635175498 41.874005383)         41.922761   \n",
       "1      POINT (-87.6318639497 41.8920421365)         41.885281   \n",
       "2      POINT (-87.6203346241 41.8571838585)         41.878866   \n",
       "3      POINT (-87.6327464887 41.8809944707)         41.892042   \n",
       "4      POINT (-87.9030396611 41.9790708201)         41.859350   \n",
       "...                                     ...               ...   \n",
       "39995   POINT (-87.6333080367 41.899602111)         41.980264   \n",
       "39996  POINT (-87.6327464887 41.8809944707)         41.877406   \n",
       "39997   POINT (-87.6333080367 41.899602111)         41.965812   \n",
       "39998  POINT (-87.9030396611 41.9790708201)         41.916005   \n",
       "39999  POINT (-87.6291051864 41.9002212967)         41.892042   \n",
       "\n",
       "      dropoff_longitude                          dropoff_location  \\\n",
       "0            -87.699155  POINT (-87.69915534320002 41.9227606205)   \n",
       "1            -87.657233      POINT (-87.6572331997 41.8852813201)   \n",
       "2            -87.625192      POINT (-87.6251921424 41.8788655841)   \n",
       "3            -87.631864      POINT (-87.6318639497 41.8920421365)   \n",
       "4            -87.617358       POINT (-87.6173580061 41.859349715)   \n",
       "...                 ...                                       ...   \n",
       "39995        -87.913625       POINT (-87.913624596 41.9802643146)   \n",
       "39996        -87.621972      POINT (-87.6219716519 41.8774061234)   \n",
       "39997        -87.655879        POINT (-87.6558787862 41.96581197)   \n",
       "39998        -87.675095      POINT (-87.6750951155 41.9160052737)   \n",
       "39999        -87.631864      POINT (-87.6318639497 41.8920421365)   \n",
       "\n",
       "       trip_start_day  trip_start_month trip_start_hour  \n",
       "0                   7                 3              23  \n",
       "1                  10                 3               4  \n",
       "2                   8                 1              11  \n",
       "3                   5                 4              17  \n",
       "4                  13                 2              15  \n",
       "...               ...               ...             ...  \n",
       "39995              12                 4               3  \n",
       "39996              26                 4              13  \n",
       "39997               3                 4              19  \n",
       "39998               6                 3              21  \n",
       "39999               8                 3               3  \n",
       "\n",
       "[40000 rows x 26 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"Final_Chicago_Train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://finalrundemo1chicago/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'finalrundemo1chicago' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n",
      "Copying file://Final_Chicago_Train.csv [Content-Type=text/csv]...\n",
      "/ [1 files][ 16.4 MiB/ 16.4 MiB]                                                \n",
      "Operation completed over 1 objects/16.4 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil mb -l {GOOGLE_CLOUD_REGION} gs://{GCS_BUCKET_NAME}\n",
    "!gsutil cp Final_Chicago_Train.csv {DATA_ROOT}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyBh_2LO4Ccw"
   },
   "source": [
    "# Write Example Component\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOjDv93eS5xV"
   },
   "source": [
    "### Write model code.\n",
    "\n",
    "We will use the same model code as in the\n",
    "[Simple TFX Pipeline Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "aES7Hv5QTDK3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "_trainer_module_file = 'bfs_trainer.py'\n",
    "_transformer_module_file = 'transformer.py'\n",
    "_training_pipeline_file = 'training_pipeline.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OB3fD-DSuAJc",
    "outputId": "2ef1dc42-31f1-4081-dd9a-0910c8208ccb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_transformer_module_file}\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "# Define the numerical features that will be used in the model.\n",
    "NUMERICAL_FEATURES = ['trip_miles', 'trip_seconds']\n",
    "\n",
    "# Define the features that will be bucketized.\n",
    "BUCKET_FEATURES = [\n",
    "    'pickup_latitude', 'pickup_longitude', 'dropoff_latitude',\n",
    "    'dropoff_longitude'\n",
    "]\n",
    "\n",
    "# Define the number of buckets used by tf.transform for encoding each feature in BUCKET_FEATURES.\n",
    "FEATURE_BUCKET_COUNT = 10\n",
    "\n",
    "# Define the categorical features that are represented as numerical values.\n",
    "CATEGORICAL_NUMERICAL_FEATURES = [\n",
    "    'trip_start_hour', 'trip_start_day', 'trip_start_month',\n",
    "    'pickup_census_tract', 'dropoff_census_tract', 'pickup_community_area',\n",
    "    'dropoff_community_area'\n",
    "]\n",
    "\n",
    "# Define the categorical features that are represented as strings.\n",
    "CATEGORICAL_STRING_FEATURES = [\n",
    "    'payment_type',\n",
    "    'company',\n",
    "]\n",
    "\n",
    "# Define the number of vocabulary terms used for encoding categorical features.\n",
    "VOCAB_SIZE = 1000\n",
    "\n",
    "# Define the count of out-of-vocab buckets in which unrecognized categorical are hashed.\n",
    "OOV_SIZE = 10\n",
    "\n",
    "# Define the keys for the label and fare columns in the input data.\n",
    "LABEL_KEY = 'fare'\n",
    "\n",
    "# Define a helper function that appends the suffix '_xf' to a feature key to avoid clashes\n",
    "# with raw feature keys when running the Evaluator component.\n",
    "def t_name(key):\n",
    "    return key + '_xf'\n",
    "\n",
    "def _make_one_hot(x, key):\n",
    "    \"\"\"Make a one-hot tensor to encode categorical features.\n",
    "    Args:\n",
    "        x: A dense tensor\n",
    "        key: A string key for the feature in the input\n",
    "    Returns:\n",
    "        A dense one-hot tensor as a float list\n",
    "    \"\"\"\n",
    "    # Computing and applying vocabulary to the input tensor and integerizing it.\n",
    "    integerized = tft.compute_and_apply_vocabulary(x,\n",
    "                                                   top_k=VOCAB_SIZE,\n",
    "                                                   num_oov_buckets=OOV_SIZE,\n",
    "                                                   vocab_filename=key,\n",
    "                                                   name=key)\n",
    "    # Getting the vocabulary size for the feature.\n",
    "    depth = (\n",
    "        tft.experimental.get_vocabulary_size_by_name(key) + OOV_SIZE)\n",
    "    # Converting the integerized tensor to a one-hot tensor.\n",
    "    one_hot_encoded = tf.one_hot(\n",
    "        integerized,\n",
    "        depth=tf.cast(depth, tf.int32),\n",
    "        on_value=1.0,\n",
    "        off_value=0.0)\n",
    "    return tf.reshape(one_hot_encoded, [-1, depth])\n",
    "\n",
    "def _fill_in_missing(x):\n",
    "    \"\"\"Replace missing values in a SparseTensor.\n",
    "    Fills in missing values of `x` with '' or 0, and converts to a dense tensor.\n",
    "    Args:\n",
    "      x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n",
    "        in the second dimension.\n",
    "    Returns:\n",
    "      A rank 1 tensor where missing values of `x` have been filled in.\n",
    "    \"\"\"\n",
    "    if not isinstance(x, tf.sparse.SparseTensor):\n",
    "        return x\n",
    "\n",
    "    default_value = '' if x.dtype == tf.string else 0\n",
    "    return tf.squeeze(\n",
    "      tf.sparse.to_dense(\n",
    "          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n",
    "          default_value),\n",
    "      axis=1)\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"tf.transform's callback function for preprocessing inputs.\n",
    "    Args:\n",
    "      inputs: map from feature keys to raw not-yet-transformed features.\n",
    "    Returns:\n",
    "      Map from string feature key to transformed feature operations.\n",
    "    \"\"\"\n",
    "    outputs = {}\n",
    "    for key in NUMERICAL_FEATURES:\n",
    "        # Filling in missing values and scaling the numerical features to have mean=0 and variance=1.\n",
    "        outputs[t_name(key)] = tft.scale_to_z_score(\n",
    "            _fill_in_missing(inputs[key]), name=key)\n",
    "\n",
    "    for key in BUCKET_FEATURES:\n",
    "        # Filling in missing values and bucketizing the features.\n",
    "        outputs[t_name(key)] = tf.cast(tft.bucketize(\n",
    "            _fill_in_missing(inputs[key]), FEATURE_BUCKET_COUNT, name=key),\n",
    "            dtype=tf.float32)\n",
    "\n",
    "    for key in CATEGORICAL_STRING_FEATURES:\n",
    "        # Filling in missing values and one-hot encoding the categorical string features.\n",
    "        outputs[t_name(key)] = _make_one_hot(_fill_in_missing(inputs[key]), key)\n",
    "\n",
    "    for key in CATEGORICAL_NUMERICAL_FEATURES:\n",
    "        # Filling in missing values, converting the categorical numerical features to strings, and one-hot encoding them.\n",
    "        outputs[t_name(key)] = _make_one_hot(tf.strings.strip(\n",
    "        tf.strings.as_string(_fill_in_missing(inputs[key]))), key)\n",
    "\n",
    "    # Fare is used as a label here\n",
    "    outputs[LABEL_KEY] = _fill_in_missing(inputs[LABEL_KEY])\n",
    "\n",
    "    return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gnc67uQNTDfW",
    "outputId": "46575526-ccab-4867-b7c5-85b85e49d8b0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bfs_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_trainer_module_file}\n",
    "# Import necessary libraries\n",
    "from typing import Dict, List, Text\n",
    "import os\n",
    "import glob\n",
    "from absl import logging\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "from tfx import v1 as tfx\n",
    "from tfx_bsl.public import tfxio\n",
    "from tensorflow_transform import TFTransformOutput\n",
    "\n",
    "\n",
    "_LABEL_KEY = 'fare'\n",
    "\n",
    "_BATCH_SIZE = 40\n",
    "\n",
    "def _input_fn(file_pattern: List[Text],\n",
    "              data_accessor: tfx.components.DataAccessor,\n",
    "              tf_transform_output: tft.TFTransformOutput,\n",
    "              batch_size: int = 200) -> tf.data.Dataset:\n",
    "    \"\"\"Generates features and label for tuning/training.\n",
    "\n",
    "    Args:\n",
    "      file_pattern: List of paths or patterns of input tfrecord files.\n",
    "      data_accessor: DataAccessor for converting input to RecordBatch.\n",
    "      tf_transform_output: A TFTransformOutput.\n",
    "      batch_size: representing the number of consecutive elements of returned\n",
    "        dataset to combine in a single batch\n",
    "\n",
    "    Returns:\n",
    "      A dataset that contains (features, indices) tuple where features is a\n",
    "        dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "    \"\"\"\n",
    "    # Create a dataset from the input files using the TFTransformOutput and batch size\n",
    "    return data_accessor.tf_dataset_factory(\n",
    "        file_pattern,\n",
    "        tfxio.TensorFlowDatasetOptions(\n",
    "            batch_size=batch_size, label_key=_LABEL_KEY),\n",
    "        tf_transform_output.transformed_metadata.schema)\n",
    "\n",
    "def _get_tf_examples_serving_signature(model, tf_transform_output):\n",
    "    \"\"\"Returns a serving signature that accepts `tensorflow.Example`.\"\"\"\n",
    "\n",
    "    # We need to track the layers in the model in order to save it.\n",
    "    # TODO(b/162357359): Revise once the bug is resolved.\n",
    "    model.tft_layer_inference = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n",
    "    ])\n",
    "    def serve_tf_examples_fn(serialized_tf_example):\n",
    "        \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
    "        raw_feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        # Remove label feature since these will not be present at serving time.\n",
    "        raw_feature_spec.pop(_LABEL_KEY)\n",
    "        raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n",
    "        transformed_features = model.tft_layer_inference(raw_features)\n",
    "        logging.info('serve_transformed_features = %s', transformed_features)\n",
    "\n",
    "        outputs = model(transformed_features)\n",
    "        # TODO(b/154085620): Convert the predicted labels from the model using a\n",
    "        # reverse-lookup (opposite of transform.py).\n",
    "        return {'outputs': outputs}\n",
    "\n",
    "    # Define a serving function that takes in serialized tf.Example and returns model outputs\n",
    "    return serve_tf_examples_fn\n",
    "\n",
    "def _get_transform_features_signature(model, tf_transform_output):\n",
    "    \"\"\"Returns a serving signature that applies tf.Transform to features.\"\"\"\n",
    "\n",
    "    # We need to track the layers in the model in order to save it.\n",
    "    # TODO(b/162357359): Revise once the bug is resolved.\n",
    "    model.tft_layer_eval = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function(input_signature=[\n",
    "        tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n",
    "    ])\n",
    "    def transform_features_fn(serialized_tf_example):\n",
    "        \"\"\"Returns the transformed_features to be fed as input to evaluator.\"\"\"\n",
    "        raw_feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n",
    "        transformed_features = model.tft_layer_eval(raw_features)\n",
    "        logging.info('eval_transformed_features = %s', transformed_features)\n",
    "        return transformed_features\n",
    "\n",
    "    # Define a serving function that takes in serialized tf.Example and returns transformed features\n",
    "    return transform_features_fn\n",
    "\n",
    "def export_serving_model(tf_transform_output, model, output_dir):\n",
    "    \"\"\"Exports a keras model for serving.\n",
    "\n",
    "    Args:\n",
    "      tf_transform_output: Wrapper around output of tf.Transform.\n",
    "      model: A keras model to export for serving.\n",
    "      output_dir: A directory where the model will be exported to.\n",
    "    \"\"\"\n",
    "    # Save the transform layer to the model for serving\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    signatures = {\n",
    "        'serving_default':\n",
    "            _get_tf_examples_serving_signature(model, tf_transform_output),\n",
    "        'transform_features':\n",
    "            _get_transform_features_signature(model, tf_transform_output),\n",
    "    }\n",
    "\n",
    "    # Save the model with serving signatures\n",
    "    model.save(output_dir, save_format='tf', signatures=signatures)\n",
    "\n",
    "def _build_keras_model(tf_transform_output: TFTransformOutput\n",
    "                       ) -> tf.keras.Model:\n",
    "    \"\"\"Creates a DNN Keras model for classifying taxi data.\n",
    "\n",
    "    Args:\n",
    "      tf_transform_output: [TFTransformOutput], the outputs from Transform\n",
    "\n",
    "    Returns:\n",
    "      A keras Model.\n",
    "    \"\"\"\n",
    "    # Create a dictionary of model inputs based on the transformed feature spec\n",
    "    feature_spec = tf_transform_output.transformed_feature_spec().copy()\n",
    "    feature_spec.pop(_LABEL_KEY)\n",
    "\n",
    "    inputs = {}\n",
    "    for key, spec in feature_spec.items():\n",
    "        if isinstance(spec, tf.io.VarLenFeature):\n",
    "            inputs[key] = tf.keras.layers.Input(shape=[None], name=key, dtype=spec.dtype, sparse=True)\n",
    "        elif isinstance(spec, tf.io.FixedLenFeature):\n",
    "            inputs[key] = tf.keras.layers.Input(shape=spec.shape or [1], name=key, dtype=spec.dtype)\n",
    "        else:\n",
    "            raise ValueError('Spec type is not supported: ', key, spec)\n",
    "\n",
    "    # Define the model architecture using the inputs\n",
    "    output = tf.keras.layers.Concatenate()(tf.nest.flatten(inputs))\n",
    "    output = tf.keras.layers.Dense(100, activation='relu')(output)\n",
    "    output = tf.keras.layers.Dense(70, activation='relu')(output)\n",
    "    output = tf.keras.layers.Dense(50, activation='relu')(output)\n",
    "    output = tf.keras.layers.Dense(20, activation='relu')(output)\n",
    "    output = tf.keras.layers.Dense(1)(output)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args: tfx.components.FnArgs):\n",
    "    \"\"\"Train the model based on given args.\n",
    "\n",
    "    Args:\n",
    "      fn_args: Holds args used to train the model as name/value pairs.\n",
    "    \"\"\"\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
    "\n",
    "    # Create training and evaluation datasets using the input function\n",
    "    train_dataset = _input_fn(fn_args.train_files, fn_args.data_accessor,\n",
    "                              tf_transform_output, _BATCH_SIZE)\n",
    "    eval_dataset = _input_fn(fn_args.eval_files, fn_args.data_accessor,\n",
    "                             tf_transform_output, _BATCH_SIZE)\n",
    "\n",
    "    # Build and compile the Keras model\n",
    "    model = _build_keras_model(tf_transform_output)\n",
    "\n",
    "    model.compile(\n",
    "      optimizer=tf.optimizers.Adam(learning_rate=0.0005), \n",
    "      loss=tf.keras.losses.MeanSquaredError(),\n",
    "      metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "    # Train the model using the training and evaluation datasets\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=fn_args.model_run_dir, update_freq='batch')\n",
    "\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch=fn_args.train_steps,\n",
    "        validation_data=eval_dataset,\n",
    "        validation_steps=fn_args.eval_steps,\n",
    "        callbacks=[tensorboard_callback])\n",
    "\n",
    "    # Export the trained model with serving signatures\n",
    "    export_serving_model(tf_transform_output, model, fn_args.serving_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-j54Ya94tR1-"
   },
   "source": [
    "### Copy files to bucket\n",
    "The transform and trainer module files need to be copied over to the GCP bucket for TFX to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rMMs5wuNYAbc",
    "outputId": "d120a9c3-c107-47d2-bf8a-961f62f1502f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://bfs_trainer.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  7.3 KiB/  7.3 KiB]                                                \n",
      "Operation completed over 1 objects/7.3 KiB.                                      \n",
      "Copying file://transformer.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  4.4 KiB/  4.4 KiB]                                                \n",
      "Operation completed over 1 objects/4.4 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp bfs_trainer.py {MODULE_ROOT}/\n",
    "!gsutil cp transformer.py {MODULE_ROOT}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-3jxJZcI7q-"
   },
   "source": [
    "### Create TFX pipeline. This pipeline can then be passed onto an orchestrator, such as KubeFlow, for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pipeline\n",
      "Creating example_gen\n",
      "Creating statistics_gen\n",
      "Creating schema_gen\n",
      "Creating example_validator\n",
      "Creating transform\n",
      "Creating trainer\n",
      "Adding model_resolver\n",
      "Adding evaluator\n",
      "Creating pusher\n",
      "Creating tfx_pipeline\n",
      "Creating runner\n",
      "Executing runner\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying transformer.py -> build/lib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "installing to /var/tmp/tmpvclegts5\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/transformer.py -> /var/tmp/tmpvclegts5\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_Transform.egg-info\n",
      "writing tfx_user_code_Transform.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Transform.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Transform.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Transform.egg-info to /var/tmp/tmpvclegts5/tfx_user_code_Transform-0.0+9a5a89b6638dd2d2f72fb9360d4a810643fc3eadbcdd9f0c43022d91098b93de-py3.10.egg-info\n",
      "running install_scripts\n",
      "creating /var/tmp/tmpvclegts5/tfx_user_code_Transform-0.0+9a5a89b6638dd2d2f72fb9360d4a810643fc3eadbcdd9f0c43022d91098b93de.dist-info/WHEEL\n",
      "creating '/var/tmp/tmpvu76go_p/tfx_user_code_Transform-0.0+9a5a89b6638dd2d2f72fb9360d4a810643fc3eadbcdd9f0c43022d91098b93de-py3-none-any.whl' and adding '/var/tmp/tmpvclegts5' to it\n",
      "adding 'transformer.py'\n",
      "adding 'tfx_user_code_Transform-0.0+9a5a89b6638dd2d2f72fb9360d4a810643fc3eadbcdd9f0c43022d91098b93de.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Transform-0.0+9a5a89b6638dd2d2f72fb9360d4a810643fc3eadbcdd9f0c43022d91098b93de.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Transform-0.0+9a5a89b6638dd2d2f72fb9360d4a810643fc3eadbcdd9f0c43022d91098b93de.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Transform-0.0+9a5a89b6638dd2d2f72fb9360d4a810643fc3eadbcdd9f0c43022d91098b93de.dist-info/RECORD'\n",
      "removing /var/tmp/tmpvclegts5\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying bfs_trainer.py -> build/lib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "installing to /var/tmp/tmp8s6kxhh5\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/bfs_trainer.py -> /var/tmp/tmp8s6kxhh5\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_Trainer.egg-info\n",
      "writing tfx_user_code_Trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Trainer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Trainer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Trainer.egg-info to /var/tmp/tmp8s6kxhh5/tfx_user_code_Trainer-0.0+0545d0a3143be72e8c4a7380209e47d36c8df9271e4bb0f0810855e6cd41d14a-py3.10.egg-info\n",
      "running install_scripts\n",
      "creating /var/tmp/tmp8s6kxhh5/tfx_user_code_Trainer-0.0+0545d0a3143be72e8c4a7380209e47d36c8df9271e4bb0f0810855e6cd41d14a.dist-info/WHEEL\n",
      "creating '/var/tmp/tmp_nuh_jqy/tfx_user_code_Trainer-0.0+0545d0a3143be72e8c4a7380209e47d36c8df9271e4bb0f0810855e6cd41d14a-py3-none-any.whl' and adding '/var/tmp/tmp8s6kxhh5' to it\n",
      "adding 'bfs_trainer.py'\n",
      "adding 'tfx_user_code_Trainer-0.0+0545d0a3143be72e8c4a7380209e47d36c8df9271e4bb0f0810855e6cd41d14a.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Trainer-0.0+0545d0a3143be72e8c4a7380209e47d36c8df9271e4bb0f0810855e6cd41d14a.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Trainer-0.0+0545d0a3143be72e8c4a7380209e47d36c8df9271e4bb0f0810855e6cd41d14a.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Trainer-0.0+0545d0a3143be72e8c4a7380209e47d36c8df9271e4bb0f0810855e6cd41d14a.dist-info/RECORD'\n",
      "removing /var/tmp/tmp8s6kxhh5\n",
      "Pipeline is ready to be executed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_pipeline(pipeline_name, pipeline_root, serving_model_dir, data_root, file_name):\n",
    "    print(\"Running pipeline\")\n",
    "\n",
    "    print(\"Creating example_gen\")\n",
    "    example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n",
    "\n",
    "    print(\"Creating statistics_gen\")\n",
    "    statistics_gen = tfx.components.StatisticsGen(examples=example_gen.outputs[\"examples\"])\n",
    "\n",
    "    print(\"Creating schema_gen\")\n",
    "    schema_gen = tfx.components.SchemaGen(statistics=statistics_gen.outputs[\"statistics\"])\n",
    "\n",
    "    print(\"Creating example_validator\")\n",
    "    example_validator = tfx.components.ExampleValidator(\n",
    "        statistics=statistics_gen.outputs[\"statistics\"],\n",
    "        schema=schema_gen.outputs[\"schema\"]\n",
    "    )\n",
    "\n",
    "    print(\"Creating transform\")\n",
    "    transform = tfx.components.Transform(\n",
    "        examples=example_gen.outputs[\"examples\"],\n",
    "        schema=schema_gen.outputs[\"schema\"],\n",
    "        module_file=os.path.join(MODULE_ROOT, \"transformer.py\")\n",
    "    )\n",
    "\n",
    "    print(\"Creating trainer\")\n",
    "    trainer = tfx.components.Trainer(\n",
    "        examples=transform.outputs[\"transformed_examples\"],\n",
    "        transform_graph=transform.outputs[\"transform_graph\"],\n",
    "        module_file=os.path.join(MODULE_ROOT, \"bfs_trainer.py\"),\n",
    "        schema=schema_gen.outputs[\"schema\"],\n",
    "        train_args=trainer_pb2.TrainArgs(num_steps=150),\n",
    "        eval_args=trainer_pb2.EvalArgs(num_steps=150)\n",
    "    )\n",
    "\n",
    "    print(\"Adding model_resolver\")\n",
    "    model_resolver = tfx.dsl.Resolver(\n",
    "        strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,\n",
    "        model=tfx.dsl.Channel(type=standard_artifacts.Model),\n",
    "        model_blessing=tfx.dsl.Channel(type=standard_artifacts.ModelBlessing)\n",
    "    ).with_id('latest_blessed_model_resolver')\n",
    "\n",
    "    print(\"Adding evaluator\")\n",
    "    evaluator = tfx.components.Evaluator(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        model=trainer.outputs['model'],\n",
    "        baseline_model=model_resolver.outputs['model'],\n",
    "        eval_config=tfma.EvalConfig(\n",
    "            model_specs=[\n",
    "                tfma.ModelSpec(\n",
    "                    signature_name='serving_default',\n",
    "                    label_key='fare',\n",
    "                    preprocessing_function_names=['transform_features'],\n",
    "                )\n",
    "            ],\n",
    "            metrics_specs=[\n",
    "                tfma.MetricsSpec(\n",
    "                    metrics=[\n",
    "                        tfma.MetricConfig(class_name='ExampleCount'),\n",
    "                        tfma.MetricConfig(class_name='MeanSquaredError')\n",
    "                    ]\n",
    "                )\n",
    "            ],\n",
    "            slicing_specs=[\n",
    "                tfma.SlicingSpec(feature_keys=['trip_start_hour'])\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"Creating pusher\")\n",
    "    pusher = tfx.components.Pusher(\n",
    "        model=trainer.outputs['model'],\n",
    "        model_blessing=evaluator.outputs['blessing'],\n",
    "        push_destination=pusher_pb2.PushDestination(\n",
    "            filesystem=pusher_pb2.PushDestination.Filesystem(base_directory=serving_model_dir)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"Creating tfx_pipeline\")\n",
    "    tfx_pipeline = Pipeline(\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_root=pipeline_root,\n",
    "        components=[\n",
    "            example_gen,\n",
    "            statistics_gen,\n",
    "            schema_gen,\n",
    "            example_validator,\n",
    "            transform,\n",
    "            trainer,\n",
    "            model_resolver,\n",
    "            evaluator,\n",
    "            pusher\n",
    "        ],\n",
    "        enable_cache=True\n",
    "    )\n",
    "\n",
    "    print(\"Creating runner\")\n",
    "    runner = tfx.orchestration.experimental.KubeflowV2DagRunner(\n",
    "        config=tfx.orchestration.experimental.KubeflowV2DagRunnerConfig(),\n",
    "        output_filename=pipeline_name + '_pipeline.json'\n",
    "    )\n",
    "\n",
    "    print(\"Executing runner\")\n",
    "    _ = runner.run(tfx_pipeline)\n",
    "\n",
    "    print(\"Pipeline is ready to be executed.\")\n",
    "    \n",
    "    \n",
    "build_pipeline(PIPELINE_NAME, PIPELINE_ROOT, SERVING_MODEL_DIR, DATA_ROOT, FILE_NAME)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Black Friday Sales pipeline using TFX",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
